name: Local Assistant
version: 1.0.0
schema: v1
models:
  - name: devstral
    provider: llama.cpp
    model: /home/ethan/.cache/llama.cpp/unsloth_Devstral-Small-2505-GGUF_Devstral-Small-2505-Q4_K_M.gguf
    apiBase: http://192.168.1.147:10000
    roles:
      - chat
      - edit
    capabilities: []
    defaultCompletionOptions:
      contextLength: 216000
      temperature: 0.15
      maxTokens: 215000
  - name: qwen
    provider: llama.cpp
    model: /home/ethan/.cache/llama.cpp/ggml-org_Qwen2.5-Coder-1.5B-Q8_0-GGUF_qwen2.5-coder-1.5b-q8_0.gguf
    apiBase: http://192.168.1.147:10001
    roles:
      - autocomplete
    requestOptions:
      extraBodyProperties:
        think: false
    defaultCompletionOptions:
      contextLength: 2500
      temperature: 0.9
      maxTokens: 1200
    autocompleteOptions:
      maxPromptTokens: 1200
      transform: false
      debounceDelay: 111
      modelTimeout: 1000
      onlyMyCode: false
      useCache: true
      useImports: true
      template: "<|fim_prefix|>{{{ prefix }}}<|fim_suffix|>{{{ suffix }}}<|fim_middle|>"
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase
